{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCZd7t5TXuUj"
      },
      "outputs": [],
      "source": [
        "# ----- installing required libraries -----\n",
        "!pip install -qU google-genai       # Google Gen AI SDK (Gemini)\n",
        "!pip install -q pdfplumber python-docx nltk gradio fpdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libs, setting up a few helpers.\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# resume handling\n",
        "import pdfplumber\n",
        "import docx\n",
        "\n",
        "# file export\n",
        "from docx import Document\n",
        "from fpdf import FPDF\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# trying to import Google GenAI client - there are two common package names depending on environment\n",
        "try:\n",
        "    # we can use google-genai / python-genai (it is newer)\n",
        "    from google import genai\n",
        "    GENAI_CLIENT_TYPE = \"google_genai\"\n",
        "except Exception:\n",
        "    try:\n",
        "        import google.generativeai as genai\n",
        "        GENAI_CLIENT_TYPE = \"google_generativeai\"\n",
        "    except Exception:\n",
        "        raise Exception(\"GenAI SDK not found. Re-run the install cell and restart runtime if needed.\")\n",
        "\n",
        "print(\"GenAI client type:\", GENAI_CLIENT_TYPE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGZfT_tZZ9Kk",
        "outputId": "7b756ec8-d546-468a-d09e-7315a627decc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenAI client type: google_genai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting my Gemini Pro API key here.\n",
        "# setting environment variable GEMINI_API_KEY in this Colab\n",
        "\n",
        "GEMINI_API_KEY = \"AIzaSyAZ3Es4SJSY2ltk8iyJNocx0gwkWf3-ke4\"  # <-- PASTING MY GEMINI PRO API KEY HERE\n",
        "\n",
        "# If I left GEMINI_API_KEY blank and I set the env var above, it's ok.\n",
        "if not GEMINI_API_KEY:\n",
        "    GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not GEMINI_API_KEY:\n",
        "    raise Exception(\"No Gemini API key found. Please paste it into GEMINI_API_KEY or set GEMINI_API_KEY environment variable.\")\n",
        "\n",
        "# Configurering the client\n",
        "if GENAI_CLIENT_TYPE == \"google_genai\":\n",
        "    # creating client; the SDK will pick the env var or you can pass key directly\n",
        "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "else:\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    client = genai  # will use name 'client' in later code for compatibility\n",
        "\n",
        "print(\"Gemini API key set. Ready to call the model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oY5L8pGaQJF",
        "outputId": "d8b291e1-9fc7-427b-94fb-d8a915e5cd7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API key set. Ready to call the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After running this cell we can upload the resume (pdf or docx).\n",
        "\n",
        "# CHANGIN THE JOB_DESCRYPTION IS MANDATORY\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your resume (PDF or DOCX).\")\n",
        "uploaded = files.upload()  # choose file from dialog\n",
        "\n",
        "# first uploaded file\n",
        "resume_filename = list(uploaded.keys())[0]\n",
        "print(\"Uploaded:\", resume_filename)\n",
        "\n",
        "# pastING the job description in the variable below (or replacing its contents).\n",
        "job_description = \"\"\"\n",
        "ML/AI Developer.\n",
        "\"\"\"\n",
        "print(\"Paste your Job Description into the 'job_description' variable in this cell and re-run if you want to edit it.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "xvdRiWDgcYK2",
        "outputId": "93afb73d-320f-416a-f1e9-8660d76d38e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your resume (PDF or DOCX).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-35760ccb-be1f-46fa-a06b-b37af9695ad7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-35760ccb-be1f-46fa-a06b-b37af9695ad7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving prabhakar rayal Resume.pdf to prabhakar rayal Resume (2).pdf\n",
            "Uploaded: prabhakar rayal Resume (2).pdf\n",
            "Paste your Job Description into the 'job_description' variable in this cell and re-run if you want to edit it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# these functions will read pdf/docx and return clean text.\n",
        "def extract_text_from_pdf(path):\n",
        "    text = []\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text.append(page.extract_text() or \"\")\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "def extract_text_from_docx(path):\n",
        "    doc = docx.Document(path)\n",
        "    text = []\n",
        "    for para in doc.paragraphs:\n",
        "        text.append(para.text)\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "def load_resume_text(filename):\n",
        "    lower = filename.lower()\n",
        "    if lower.endswith(\".pdf\"):\n",
        "        return extract_text_from_pdf(filename)\n",
        "    elif lower.endswith(\".docx\") or lower.endswith(\".doc\"):\n",
        "        return extract_text_from_docx(filename)\n",
        "    else:\n",
        "        raise Exception(\"Unsupported resume format. Use PDF or DOCX.\")\n",
        "\n",
        "# Loading the resume text here\n",
        "resume_text = load_resume_text(resume_filename)\n",
        "print(\"----- Your Resume Preview (first 600 chars) -----\")\n",
        "print(resume_text[:600] + \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hjy8zvIldC3j",
        "outputId": "515519d2-6bcd-4081-cd2b-d85d9ced600d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Your Resume Preview (first 600 chars) -----\n",
            "Prabhakar Rayal\n",
            "Email-id : prabhakarrayalarcy@gmail.com\n",
            "Mobile No.: 9027677731,\n",
            "www.linkedin.com/in/prabhakar-rayal-663968259\n",
            "EDUCATION\n",
            "• B.Tech in Computer Science | Graphic Era Hill University, Dehradun\n",
            "2021 – 2025 | CGPA: 7.59/10\n",
            "• 12th Grade (I.S.E) | Modern School, Rishikesh\n",
            "2020 – 2021 | Percentage: 69.5%\n",
            "• 10th Grade (I.C.S.E) | Modern School, Rishikesh\n",
            "2018 – 2019 | Percentage: 60.4%\n",
            "PROJECTS\n",
            "• BGMI Tournament Website (HTML, CSS, JavaScript)\n",
            "Feb 2022 – Apr 2022\n",
            "Developed a web platform for BGMI players to register and participate in custom tournaments (frontend).\n",
            "• Movie Recommender Sy...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libs that we will be needing below\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34hBZU3prmeP",
        "outputId": "12c7bc77-a816-436d-d58b-4e4b662cda13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting likely skills/keywords from JD using simple guideline.\n",
        "def extract_skills_from_text(text, top_k=40):\n",
        "    # simple tokenization, will keep word tokens that look like skills (alphanumeric + punctuation)\n",
        "    tokens = [w.lower() for w in word_tokenize(text) if len(w) > 1]\n",
        "    # removing stopwords-ish list (simple)\n",
        "    stop = set([\"the\",\"and\",\"with\",\"for\",\"of\",\"to\",\"in\",\"a\",\"on\",\"is\",\"experience\",\"years\",\"year\",\"should\",\"strong\"])\n",
        "    tokens = [t for t in tokens if t.isalnum() and t not in stop]\n",
        "    # will capture common multiword skills via simple patterns (like 'machine learning', 'deep learning')\n",
        "    multi_phrases = []\n",
        "    lowered = text.lower()\n",
        "    for phrase in [\"machine learning\",\"deep learning\",\"data science\",\"computer vision\",\"natural language processing\",\"nlp\",\"react js\",\"node js\",\"git\",\"docker\",\"kubernetes\",\"flask\",\"django\",\"fastapi\",\"tensorflow\",\"pytorch\",\"rest api\",\"sql\",\"nosql\",\"mongodb\",\"postgresql\",\"aws\",\"gcp\",\"azure\",\"javascript\",\"html\",\"css\",\"c++\",\"c#\",\"java\",\"python\"]:\n",
        "        if phrase in lowered:\n",
        "            multi_phrases.append(phrase)\n",
        "    # will get most common single tokens\n",
        "    most = [t for t,c in Counter(tokens).most_common(top_k)]\n",
        "    # combinining\n",
        "    skills = list(dict.fromkeys(multi_phrases + most))  # will preserve order, remove dupes\n",
        "    return skills\n",
        "\n",
        "jd_skills = extract_skills_from_text(job_description)\n",
        "print(\"Extracted JD skills (top):\", jd_skills[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOFFDU3meIZq",
        "outputId": "a3c5b77b-b4c0-413c-c732-9c5f63a60f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted JD skills (top): ['developer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finding which JD skills are missing from the resume\n",
        "def find_missing_skills(resume_text, skills_list):\n",
        "    text = resume_text.lower()\n",
        "    missing = [s for s in skills_list if s not in text]\n",
        "    present = [s for s in skills_list if s in text]\n",
        "    return present, missing\n",
        "\n",
        "present_skills, missing_skills = find_missing_skills(resume_text, jd_skills)\n",
        "print(\"Present skills (sample):\", present_skills[:15])\n",
        "print(\"Missing skills (sample):\", missing_skills[:15])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "397F0ocuqz4R",
        "outputId": "00dea5d5-a53a-48ca-b52f-6cee2231dad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Present skills (sample): []\n",
            "Missing skills (sample): ['developer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# these prompts are like the heart of the tool.\n",
        "# Here we can also tweak the tone below (like - professional, friendly, concise).\n",
        "analysis_prompt = f\"\"\"\n",
        "You are an ATS resume optimisation assistant.\n",
        "First, detect whether the resume is technical or non-technical.\n",
        "- If technical, focus on adding role-specific hard skills, tools, and frameworks.\n",
        "- If non-technical, focus on soft skills, relevant industry experience, and domain-specific keywords.\n",
        "Do not insert irrelevant terms.\n",
        "\n",
        "Resume text:\n",
        "{resume_text}\n",
        "\n",
        "Job description:\n",
        "{job_description}\n",
        "\n",
        "Step 1: Classify resume as technical or non-technical.\n",
        "Step 2: Identify missing skills and keywords relevant to this domain.\n",
        "Step 3: Suggest modifications for ATS optimisation.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_ANALYZE = \"\"\"\n",
        "You are a professional resume analyst. Given the candidate's resume text and a job description,\n",
        "1) list missing or weakly-present skills from the JD that should be added,\n",
        "2) suggest concise bullet points to add in Experience or Projects to reflect those skills (write 2-3 sample bullets each),\n",
        "3) give an estimated ATS keyword match percentage (simple estimate).\n",
        "\n",
        "Resume:\n",
        "{resume}\n",
        "\n",
        "Job Description:\n",
        "{jd}\n",
        "\n",
        "Return the result as JSON with fields: missing_skills, suggestions (list), ats_estimate (0-100), explanation (short).\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_REWRITE = \"\"\"\n",
        "You are a professional resume writer. Rewrite the candidate's resume to match the given Job Description.\n",
        "- Keep the candidate's facts (roles, company names, dates, project titles) intact.\n",
        "- Improve wording, make bullets achievement-focused, add missing skills where appropriate (based on suggestions).\n",
        "- Keep output in clean resume-format text with sections: Summary, Skills, Experience (with bullets), Projects, Education.\n",
        "- Use a professional, concise tone suitable for tech job applications.\n",
        "\n",
        "Resume (original):\n",
        "{resume}\n",
        "\n",
        "Job Description:\n",
        "{jd}\n",
        "\n",
        "Missing skills suggestions (optional, JSON list):\n",
        "{suggestions}\n",
        "\n",
        "Produce only the rewritten resume text (no extra commentary).\n",
        "\"\"\"\n",
        "\n",
        "# later we will fill in {resume}, {jd}, {suggestions}\n"
      ],
      "metadata": {
        "id": "pqzf-oU-rOKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell sends the analysis prompt to Gemini and gets back the missing-skills + suggestions.\n",
        "# We use a short system and user style prompt. We can also adjust model name if needed (like - gemini-1.5+/gemini-2.5-flash, etc).\n",
        "\n",
        "MODEL_NAME = \"gemini-2.5-pro\"  # I have Gemini pro, so i am using the pro version.\n",
        "\n",
        "analysis_prompt = PROMPT_ANALYZE.format(resume=resume_text[:5000], jd=job_description[:5000])  # we can cut to safe length\n",
        "\n",
        "print(\"Sending analysis prompt to Gemini... (this may take a few seconds)\")\n",
        "if GENAI_CLIENT_TYPE == \"google_genai\":\n",
        "    # this is new client style\n",
        "    response = client.models.generate_content(model=MODEL_NAME, contents=analysis_prompt)\n",
        "    raw_text = response.text\n",
        "else:\n",
        "    # this is older style\n",
        "    resp = client.generate_text(model=MODEL_NAME, prompt=analysis_prompt)\n",
        "    raw_text = resp.text if hasattr(resp, 'text') else str(resp)\n",
        "\n",
        "print(\"Raw analysis output (preview):\")\n",
        "print(raw_text[:5000])\n",
        "\n",
        "# Here we will attempt to parse JSON from output (many times Gemini returns readable text)\n",
        "import re, json\n",
        "json_text = None\n",
        "try:\n",
        "    # it tries to pull JSON object from text\n",
        "    m = re.search(r\"\\{[\\s\\S]*\\}\", raw_text)\n",
        "    if m:\n",
        "        json_text = json.loads(m.group(0))\n",
        "except Exception as e:\n",
        "    print(\"Couldn't parse JSON automatically. We'll fall back to heuristics.\")\n",
        "    json_text = None\n",
        "\n",
        "if json_text:\n",
        "    suggestions = json_text.get(\"suggestions\", [])\n",
        "    missing_skills_from_ai = json_text.get(\"missing_skills\", [])\n",
        "    ats_estimate = json_text.get(\"ats_estimate\", \"N/A\")\n",
        "else:\n",
        "    #  it will fallback and use our earlier missing_skills and create simple bullets\n",
        "    missing_skills_from_ai = missing_skills\n",
        "    suggestions = []\n",
        "    for s in missing_skills_from_ai[:8]:\n",
        "        suggestions.append(f\"- Add experience / project bullet showing experience with {s} (quantify impact).\")\n",
        "    ats_estimate = None\n",
        "\n",
        "print(\"Missing skills (final):\", missing_skills_from_ai[:20])\n",
        "print(\"Sample suggestions:\", suggestions[:6])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBMKNBxh3z6W",
        "outputId": "9fb566b4-142d-48ae-a28a-c30ea26bbca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending analysis prompt to Gemini... (this may take a few seconds)\n",
            "Raw analysis output (preview):\n",
            "```json\n",
            "{\n",
            "  \"missing_skills\": [\n",
            "    \"Deep Learning Frameworks (PyTorch, TensorFlow)\",\n",
            "    \"Model Evaluation Metrics (e.g., Accuracy, Precision, Recall, PSNR, MSE)\",\n",
            "    \"Model Deployment & API Development (Using frameworks like Flask)\",\n",
            "    \"Advanced ML/DL Architectures (e.g., CNNs for image tasks)\",\n",
            "    \"Version Control (Git, GitHub)\"\n",
            "  ],\n",
            "  \"suggestions\": [\n",
            "    {\n",
            "      \"skill_to_add\": \"Deep Learning Frameworks & Advanced Architectures\",\n",
            "      \"bullet_points\": [\n",
            "        \"Major Project: Implemented and trained Convolutional Neural Network (CNN) based autoencoders using TensorFlow/PyTorch to effectively denoise medical image data.\",\n",
            "        \"Major Project: Engineered and compared multiple deep learning models, leveraging autoencoder architectures to significantly improve image clarity and data quality for diagnostic purposes.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"skill_to_add\": \"Model Evaluation & Quantifiable Metrics\",\n",
            "      \"bullet_points\": [\n",
            "        \"Major Project: Evaluated model performance using Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE), achieving a 20% higher PSNR compared to traditional filtering methods.\",\n",
            "        \"Movie Recommender System: Validated the recommendation engine's effectiveness by achieving an 85% precision rate for top-5 movie suggestions on a held-out test dataset.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"skill_to_add\": \"Model Deployment & API Development\",\n",
            "      \"bullet_points\": [\n",
            "        \"Updated Medicine Chatbot: Developed and integrated a REST API using Flask to serve model predictions, connecting the Python-based ML backend to the user-facing chatbot interface.\",\n",
            "        \"Updated Medicine Chatbot: Packaged the machine learning model within a Flask application, enabling real-time inference and responses for user symptom queries.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"skill_to_add\": \"Version Control\",\n",
            "      \"bullet_points\": [\n",
            "        \"Add 'Git & GitHub' to your 'Tools & IDEs' section to explicitly list this crucial skill.\",\n",
            "        \"Consider adding a GitHub profile link next to your LinkedIn link at the top of the resume to showcase your project code.\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"ats_estimate\": 55,\n",
            "  \"explanation\": \"The resume has a solid foundation with Python, core ML libraries (Sklearn, Pandas), and relevant project themes. However, it lacks crucial industry-standard keywords like TensorFlow/PyTorch, specific model architectures (CNN), and quantifiable performance metrics. Adding these, along with showing deployment experience (e.g., with Flask), will significantly increase its match rate for ML/AI developer roles.\"\n",
            "}\n",
            "```\n",
            "Missing skills (final): ['Deep Learning Frameworks (PyTorch, TensorFlow)', 'Model Evaluation Metrics (e.g., Accuracy, Precision, Recall, PSNR, MSE)', 'Model Deployment & API Development (Using frameworks like Flask)', 'Advanced ML/DL Architectures (e.g., CNNs for image tasks)', 'Version Control (Git, GitHub)']\n",
            "Sample suggestions: [{'skill_to_add': 'Deep Learning Frameworks & Advanced Architectures', 'bullet_points': ['Major Project: Implemented and trained Convolutional Neural Network (CNN) based autoencoders using TensorFlow/PyTorch to effectively denoise medical image data.', 'Major Project: Engineered and compared multiple deep learning models, leveraging autoencoder architectures to significantly improve image clarity and data quality for diagnostic purposes.']}, {'skill_to_add': 'Model Evaluation & Quantifiable Metrics', 'bullet_points': ['Major Project: Evaluated model performance using Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE), achieving a 20% higher PSNR compared to traditional filtering methods.', \"Movie Recommender System: Validated the recommendation engine's effectiveness by achieving an 85% precision rate for top-5 movie suggestions on a held-out test dataset.\"]}, {'skill_to_add': 'Model Deployment & API Development', 'bullet_points': ['Updated Medicine Chatbot: Developed and integrated a REST API using Flask to serve model predictions, connecting the Python-based ML backend to the user-facing chatbot interface.', 'Updated Medicine Chatbot: Packaged the machine learning model within a Flask application, enabling real-time inference and responses for user symptom queries.']}, {'skill_to_add': 'Version Control', 'bullet_points': [\"Add 'Git & GitHub' to your 'Tools & IDEs' section to explicitly list this crucial skill.\", 'Consider adding a GitHub profile link next to your LinkedIn link at the top of the resume to showcase your project code.']}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now here weare asking Gemini to rewrite the resume using suggestions from the analysis step.\n",
        "rewrite_prompt = PROMPT_REWRITE.format(resume=resume_text[:6000], jd=job_description[:4000], suggestions=json.dumps(suggestions))\n",
        "\n",
        "print(\"Sending rewrite request to Gemini... (this will be the biggest call)\")\n",
        "if GENAI_CLIENT_TYPE == \"google_genai\":\n",
        "    response = client.models.generate_content(model=MODEL_NAME, contents=rewrite_prompt)\n",
        "    rewritten = response.text\n",
        "else:\n",
        "    resp = client.generate_text(model=MODEL_NAME, prompt=rewrite_prompt)\n",
        "    rewritten = resp.text if hasattr(resp, 'text') else str(resp)\n",
        "\n",
        "print(\"----- Rewritten resume preview -----\")\n",
        "print(rewritten[:6000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WTcV8M-7tJQ",
        "outputId": "45640512-6f9b-405f-f29c-3f4e4b3a9b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending rewrite request to Gemini... (this will be the biggest call)\n",
            "----- Rewritten resume preview -----\n",
            "**Prabhakar Rayal**\n",
            "prabhakarrayalarcy@gmail.com | (902) 767-7731 | LinkedIn: /in/prabhakar-rayal | GitHub: [Your GitHub URL]\n",
            "\n",
            "---\n",
            "\n",
            "### **Summary**\n",
            "\n",
            "Driven and analytical Computer Science undergraduate specializing in Machine Learning and Artificial Intelligence. Proficient in Python, deep learning frameworks, and end-to-end model development, from data processing to API deployment. Seeking to leverage hands-on project experience in building and optimizing AI-powered solutions to contribute to an innovative ML/AI Developer role.\n",
            "\n",
            "---\n",
            "\n",
            "### **Skills**\n",
            "\n",
            "*   **Programming Languages:** Python, Java, C++, SQL, JavaScript, HTML/CSS\n",
            "*   **ML/AI Libraries & Frameworks:** Scikit-learn, Pandas, NumPy, TensorFlow, PyTorch, Tkinter\n",
            "*   **Tools & Technologies:** Git, GitHub, Flask, VS Code, Google Colab, Jupyter Notebooks\n",
            "*   **Cloud & Databases:** AWS (Basics), SQL\n",
            "*   **Languages:** English (Fluent), Hindi (Fluent)\n",
            "\n",
            "---\n",
            "\n",
            "### **Experience**\n",
            "\n",
            "**Major Project - Medical Image Denoising using Machine Learning** | Jan 2025 – Jun 2025\n",
            "*   Engineered and compared multiple deep learning models, leveraging Convolutional Neural Network (CNN) based autoencoder architectures in TensorFlow/PyTorch to significantly improve image clarity.\n",
            "*   Implemented various filtering and ML-based denoising techniques on grayscale medical images to enhance data quality for diagnostic purposes.\n",
            "*   Evaluated model performance using Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE), achieving a 20% higher PSNR compared to traditional filtering methods.\n",
            "\n",
            "**Updated Medicine Chatbot** | May 2024 – Jul 2024\n",
            "*   Developed a full-stack chatbot application that recommends over-the-counter medicines based on user-described symptoms, using Python for the ML backend and HTML/CSS/JavaScript for the UI.\n",
            "*   Packaged the machine learning model and developed a REST API using Flask to serve predictions, enabling real-time inference and responses for user symptom queries.\n",
            "*   Expanded the initial dataset and model features to improve recommendation accuracy and user experience.\n",
            "\n",
            "**Movie Recommender System** | Nov 2022 – Feb 2023\n",
            "*   Built and trained a content-based movie recommendation engine using Python, leveraging Pandas for data manipulation and Scikit-learn for implementing cosine similarity.\n",
            "*   Processed and vectorized a movie dataset to create a feature-based similarity matrix for generating personalized recommendations.\n",
            "*   Validated the recommendation engine's effectiveness by achieving an 85% precision rate for top-5 movie suggestions on a held-out test dataset.\n",
            "\n",
            "**Supermarket Billing System** | Apr 2023 – Jul 2023\n",
            "*   Designed and developed a comprehensive billing system in C++ to streamline point-of-sale operations.\n",
            "*   Automated key processes including transaction recording, real-time inventory tracking, and generation of customer purchase logs to improve efficiency.\n",
            "\n",
            "---\n",
            "\n",
            "### **Projects**\n",
            "\n",
            "**BGMI Tournament Website** | Feb 2022 – Apr 2022\n",
            "*   Developed a responsive front-end web platform using HTML, CSS, and JavaScript to allow users to register for and view details of gaming tournaments.\n",
            "\n",
            "---\n",
            "\n",
            "### **Education**\n",
            "\n",
            "**B.Tech in Computer Science** | Graphic Era Hill University, Dehradun | 2021 – 2025\n",
            "*   CGPA: 7.59/10\n",
            "\n",
            "---\n",
            "\n",
            "### **Certifications & Professional Development**\n",
            "\n",
            "*   **Cloud Computing Basics (AWS)** – Infosys\n",
            "*   **Cyber Security Awareness Program** – Infotech\n",
            "*   **Participant, Hackathon 1.0 & University Web Development Competitions**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saveing the rewritten resume into a neat DOCX file for download.\n",
        "# Adding cleaning so PDF export won't fail with special characters.\n",
        "\n",
        "def text_to_docx(text, out_filename=\"optimized_resume.docx\"):\n",
        "    doc = Document()\n",
        "    # will break text into chunks by double newlines for basic structure\n",
        "    parts = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "    for p in parts:\n",
        "        # Heuristic will detect headings (short text, all caps, or section keywords)\n",
        "        if len(p) < 80 and (p.isupper() or any(h.lower() in p.lower() for h in\n",
        "            [\"summary\", \"skills\", \"experience\", \"education\", \"projects\"])):\n",
        "            doc.add_heading(p.strip(), level=2)\n",
        "        else:\n",
        "            for line in p.split(\"\\n\"):\n",
        "                if line.strip().startswith(\"-\") or line.strip().startswith(\"•\"):\n",
        "                    doc.add_paragraph(line.strip().lstrip(\"-• \"), style='List Bullet')\n",
        "                else:\n",
        "                    doc.add_paragraph(line.strip())\n",
        "    doc.save(out_filename)\n",
        "    return out_filename\n",
        "\n",
        "# Save DOCX\n",
        "out_docx = text_to_docx(rewritten, out_filename=\"optimized_resume.docx\")\n",
        "print(\"Saved DOCX:\", out_docx)\n",
        "\n",
        "# Simple PDF export: clean up fancy characters so FPDF (latin-1) doesn't crash\n",
        "import re\n",
        "\n",
        "def clean_for_pdf(text):\n",
        "    # wWill replace fancy quotes and dashes with normal ones\n",
        "    replacements = {\n",
        "        \"–\": \"-\", \"—\": \"-\", \"’\": \"'\", \"‘\": \"'\", \"“\": '\"', \"”\": '\"'\n",
        "    }\n",
        "    for bad, good in replacements.items():\n",
        "        text = text.replace(bad, good)\n",
        "    # Will remove any other non-latin1 chars\n",
        "    return text.encode('latin-1', 'replace').decode('latin-1')\n",
        "\n",
        "# This will convert DOCX text to PDF (basic formatting)\n",
        "from fpdf import FPDF\n",
        "def docx_to_pdf_simple(docx_text, out_pdf=\"optimized_resume.pdf\"):\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    safe_text = clean_for_pdf(docx_text)\n",
        "    for line in safe_text.split(\"\\n\"):\n",
        "        pdf.multi_cell(0, 7, txt=line)\n",
        "    pdf.output(out_pdf)\n",
        "    return out_pdf\n",
        "\n",
        "# PDF save\n",
        "txt = rewritten\n",
        "out_pdf = docx_to_pdf_simple(txt, out_pdf=\"optimized_resume.pdf\")\n",
        "print(\"Saved PDF:\", out_pdf)\n",
        "\n",
        "# Provide download links in Colab\n",
        "from google.colab import files as colab_files\n",
        "print(\"You can download the files now:\")\n",
        "colab_files.download(\"optimized_resume.docx\")\n",
        "colab_files.download(\"optimized_resume.pdf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "uA17z_Ud8r2c",
        "outputId": "d7d86f1e-218f-4f69-b9e2-b157f55e33b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved DOCX: optimized_resume.docx\n",
            "Saved PDF: optimized_resume.pdf\n",
            "You can download the files now:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c2f62394-3fb8-4851-bd49-76bff8561c38\", \"optimized_resume.docx\", 38433)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0978fd5b-264d-417e-81cb-03c40d4f9331\", \"optimized_resume.pdf\", 3978)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a small JSON report summarizing changes for HR (they asked for explanation)\n",
        "report = {\n",
        "    \"original_resume_file\": resume_filename,\n",
        "    \"optimized_resume_file\": \"optimized_resume.docx\",\n",
        "    \"missing_skills_added\": missing_skills_from_ai,\n",
        "    \"suggestions_applied\": suggestions,\n",
        "    \"ats_estimate_before\": None,   # you could add an estimate before by simple matching\n",
        "    \"ats_estimate_after\": ats_estimate\n",
        "}\n",
        "\n",
        "with open(\"resume_change_report.json\", \"w\") as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(\"Saved report: resume_change_report.text\")\n",
        "colab_files.download(\"resume_change_report.txt\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NqXk5S99F03d",
        "outputId": "645dbc8c-54cd-4e4a-d954-9596d9f3950b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved report: resume_change_report.text\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bab64ca9-34fa-4b96-ab21-6fbd34cdc334\", \"resume_change_report.txt\", 1965)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#📄 ATS Resume Optimizer\n",
        "\n",
        "An AI-powered tool that rewrites resumes to maximize Applicant Tracking System (ATS) match scores by aligning them with a given job description.\n",
        "This project uses Google Gemini API for text analysis and rewriting, extracting missing skills, and producing an optimized, ATS-friendly version of the resume.\n",
        "\n",
        "#🚀 Features\n",
        "\n",
        "* Automatic Skill Extraction\n",
        "\n",
        "Detects missing keywords, skills, and phrases from the job description.\n",
        "\n",
        "* Technical & Non-Technical Resume Handling\n",
        "\n",
        "Classifies resumes as technical or non-technical and optimizes accordingly.\n",
        "\n",
        "* ATS Match Estimation\n",
        "\n",
        "Provides an estimated ATS keyword match score before and after optimization.\n",
        "\n",
        "* Professional Rewriting\n",
        "\n",
        "Improves grammar, tone, and structure while keeping facts intact.\n",
        "\n",
        "* DOCX & PDF Export\n",
        "\n",
        "Outputs clean, recruiter-ready resumes in both .docx and .pdf formats.\n",
        "\n",
        "* Change Report\n",
        "\n",
        "Generates a JSON report summarizing modifications for transparency.\n",
        "\n",
        "#🛠️ Tech Stack\n",
        "\n",
        "    Language: Python 3.\n",
        "\n",
        "    AI API: Google Gemini API. (THE API KEY I USED IN THIS PROJECT WILL BE AVAILABLE FOR 7 DAYS AFTER SUBMISSION)\n",
        "\n",
        "    Libraries: google-generativeai (Gemini API client),\n",
        "\n",
        "    nltk (skill extraction), python-docx (DOCX creation),\n",
        "\n",
        "    fpdf (PDF generation), re (text cleaning).\n",
        "\n",
        "#📂 Project Structure\n",
        "\n",
        "resume-optimizer/\n",
        "\n",
        "├── Resume_Optimizer.ipynb       # Main notebook\n",
        "\n",
        "├── requirements.txt             # Dependencies\n",
        "\n",
        "├── sample_resume.docx           # Example resume\n",
        "\n",
        "├── job_description.txt          # Example job\n",
        "description\n",
        "\n",
        "├── optimized_resume.docx        # Output DOCX\n",
        "\n",
        "├── optimized_resume.pdf         # Output PDF\n",
        "\n",
        "├── resume_change_report.json    # Summary of changes\n",
        "\n",
        "└── README.md # Project documentation\n",
        "\n",
        "    ⚙️ Setup Instructions\n",
        "\n",
        "1️⃣ Clone the Repository\n",
        "\n",
        "git clone https://github.com/Prabhakarrayal\n",
        "\n",
        "    cd resume-optimizer\n",
        "\n",
        "2️⃣ Install Dependencies\n",
        "\n",
        "\n",
        "    pip install -r requirements.txt\n",
        "\n",
        "3️⃣ Set up Google Gemini API Key\n",
        "\n",
        "Go to Google AI Studio\n",
        "\n",
        "Generate an API key\n",
        "Save it in a .env file:\n",
        "\n",
        "    GEMINI_API_KEY=your_api_key_here\n",
        "\n",
        "4️⃣ Run the Notebook\n",
        "In Google Colab or VS Code Jupyter Extension:\n",
        "\n",
        "\n",
        "    jupyter notebook Resume_Optimizer.ipynb\n",
        "\n",
        "#🖥️ Usage\n",
        "\n",
        "Upload Resume (DOCX or TXT format)\n",
        "\n",
        "Paste Job Description in the input cell\n",
        "\n",
        "Run All Cells\n",
        "\n",
        "Download Optimized Resume & Report\n",
        "\n",
        "#📊 Example Output\n",
        "ATS Match Before: 62%\n",
        "\n",
        "ATS Match After: 88%\n",
        "\n",
        "Added Skills: [\"Agile Methodology\", \"SQL\", \"Stakeholder Management\"]\n",
        "\n",
        "#📎 Resources\n",
        "Google Gemini API Docs\n",
        "\n",
        "Python-docx Documentation\n",
        "\n",
        "FPDF Documentation\n",
        "\n",
        "NLTK Documentation\n",
        "\n"
      ],
      "metadata": {
        "id": "vAEPuwzDFL0t"
      }
    }
  ]
}